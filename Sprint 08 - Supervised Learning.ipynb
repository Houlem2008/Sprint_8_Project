{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Thanks for taking the time to improve the project! It is now accepted. Keep up the good work on the next sprint!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review**\n",
    "\n",
    "Hi, my name is Dmitry and I will be reviewing your project.\n",
    "  \n",
    "You can find my comments in colored markdown cells:\n",
    "  \n",
    "<div class=\"alert alert-success\">\n",
    "  If everything is done successfully.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-warning\">\n",
    "  If I have some (optional) suggestions, or questions to think about, or general comments.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-danger\">\n",
    "  If a section requires some corrections. Work can't be accepted with red comments.\n",
    "</div>\n",
    "  \n",
    "Please don't remove my comments, as it will make further review iterations much harder for me.\n",
    "  \n",
    "Feel free to reply to my comments or ask questions using the following template:\n",
    "  \n",
    "<div class=\"alert alert-info\">\n",
    "  For your comments and questions.\n",
    "</div>\n",
    "  \n",
    "First of all, thank you for turning in the project! You did a pretty good job overall, but there are some problems that need to be fixed before the project is accepted. Let me know if you have questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Bank Customer Retention Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having been provided data from Beta Bank, we will create a number of predictive models and select the best one to help determine whether customers will leave Beta Bank in the near future. Our threshold for minimum acceptance while creating the models and selecting one will be an F-score of 0.59 which is the relationship between `recall`, a model's abililty to correctly identify True Positive predictions while avoiding False Negatives, and `precision`, a model's ability to correctly identify True Positive predictions while avoiding False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber            0\n",
       "CustomerId           0\n",
       "Surname              0\n",
       "CreditScore          0\n",
       "Geography            0\n",
       "Gender               0\n",
       "Age                  0\n",
       "Tenure             909\n",
       "Balance              0\n",
       "NumOfProducts        0\n",
       "HasCrCard            0\n",
       "IsActiveMember       0\n",
       "EstimatedSalary      0\n",
       "Exited               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Tenure'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber          0\n",
       "CustomerId         0\n",
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tenure` contained 909 missing values, and when compared to the 10,000 rows of data, they represented just over 9% of the provided data, and so rather than filling the values with any kind of approximation, the rows were removed to help maintain data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Ok, that's one way to deal with missing values!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quality of Life Alterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns= df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9091 entries, 0 to 9998\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   rownumber        9091 non-null   int64  \n",
      " 1   customerid       9091 non-null   int64  \n",
      " 2   surname          9091 non-null   object \n",
      " 3   creditscore      9091 non-null   int64  \n",
      " 4   geography        9091 non-null   object \n",
      " 5   gender           9091 non-null   object \n",
      " 6   age              9091 non-null   int64  \n",
      " 7   tenure           9091 non-null   float64\n",
      " 8   balance          9091 non-null   float64\n",
      " 9   numofproducts    9091 non-null   int64  \n",
      " 10  hascrcard        9091 non-null   int64  \n",
      " 11  isactivemember   9091 non-null   int64  \n",
      " 12  estimatedsalary  9091 non-null   float64\n",
      " 13  exited           9091 non-null   int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names were changed to lowercase for consistency in coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     952\n",
       "2.0     950\n",
       "8.0     933\n",
       "3.0     928\n",
       "5.0     927\n",
       "7.0     925\n",
       "4.0     885\n",
       "9.0     882\n",
       "6.0     881\n",
       "10.0    446\n",
       "0.0     382\n",
       "Name: tenure, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tenure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tenure'] = df['tenure'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     952\n",
       "2     950\n",
       "8     933\n",
       "3     928\n",
       "5     927\n",
       "7     925\n",
       "4     885\n",
       "9     882\n",
       "6     881\n",
       "10    446\n",
       "0     382\n",
       "Name: tenure, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tenure'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tenure` values were changed to integer data type to assist with reducing the storage size and help with processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Modelling Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stratified split into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.2, random_state=12345, stratify=df['exited'] )\n",
    "\n",
    "train, test = train_test_split(train, test_size=0.25, random_state=12345, stratify=train['exited'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split the data into three sets; training, validation, and testing, at a 3:1:1 ratio. First we took 20% of the data to create the validation set, then from the remaining 80%, we took 25% to make the testing set. This follows the ratio of 3:1:1 or 60%, 20%, and 20% since after the first split, 80% of the initial data remained, and 0.8 * 0.25 = 0.2, representing a second 20% share of the full data set.\n",
    "\n",
    "The sets were also stratified on the `exited` column which is the target - this will ensure that there is an equal proportion as to the original dataset in the three new splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check geography for viability of inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "France     4550\n",
       "Germany    2293\n",
       "Spain      2248\n",
       "Name: geography, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['geography'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 3 different values, `geography` will not need to get dropped but rather converted to a numeric representation for use with a ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create features and target sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = train.drop(['rownumber', 'customerid', 'surname', 'exited'] ,axis=1)\n",
    "target_train = train['exited']\n",
    "features_valid = valid.drop(['rownumber', 'customerid', 'surname', 'exited'] ,axis=1)\n",
    "target_valid = valid['exited']\n",
    "features_test = test.drop(['rownumber', 'customerid', 'surname', 'exited'] ,axis=1)\n",
    "target_test = test['exited']\n",
    "\n",
    "features_train = pd.get_dummies(features_train, drop_first=True)\n",
    "features_valid = pd.get_dummies(features_valid, drop_first=True)\n",
    "features_test = pd.get_dummies(features_test, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, validation, and testing sets have been split into features and target sets. One-Hot Encoding was also used to turn columns that weren't previously usable into usable ones. The dummy trap has also been taken into account and extraneous columns created through OHE have been dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Categorical features were encoded; good idea to drop uninformative columns\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check target distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    1112\n",
       "Name: exited, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1448\n",
       "1     371\n",
       "Name: exited, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1447\n",
       "1     371\n",
       "Name: exited, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(target_train.value_counts())\n",
    "display(target_valid.value_counts())\n",
    "display(target_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are approximately four times the number of customers who did not leave opposed to those that did leave Beta Bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The data was split into train,, validation and test sets reasonably\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment</b>\n",
    "\n",
    "Could you check the target distribution? Otherwise it's not clear why we're talking about imbalance at all :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  Checked the value counts in the newly shuffled 3.2.4 section.\n",
    "    <p></p>\n",
    "  Because of an immense amount of shuffling in the order above, I definitely messed with where your comments should line up properly. I did however stratify the data so there should be the proper proportion in each of the sets and verified it as well with value counts for each of the three sets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Ok, no problem!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of best model: 7\n",
      "F1 score of the decision tree model on the validation set: 0.5745682888540031\n",
      "AUC_ROC score of the decision tree model on the validation set: 0.681033231076231\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_result = 0\n",
    "for depth in range(1, 50):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=depth) # create a model with the given depth\n",
    "    model.fit(features_train, target_train) # train the model\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    result = f1_score(target_valid, predictions_valid)\n",
    "    if result > best_result:\n",
    "        best_model = model\n",
    "        best_result = result\n",
    "        best_depth = depth\n",
    "        \n",
    "predictions_test = best_model.predict(features_valid)\n",
    "test_result = accuracy_score(target_valid, predictions_valid)\n",
    "\n",
    "print(\"Depth of best model:\", best_depth)\n",
    "print(\"F1 score of the decision tree model on the validation set:\", best_result)\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print(\"AUC_ROC score of the decision tree model on the validation set:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator value of best model: 41\n",
      "F1 score of the random forest on the model validation set: 0.5874799357945426\n",
      "AUC_ROC score of the random forest model on the validation set: 0.8389255186073179\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 50): # choose hyperparameter range\n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators=est) # set number of trees\n",
    "    model.fit(features_train, target_train) # train model on training set\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    score = f1_score(target_valid, predictions_valid)    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_est = est\n",
    "\n",
    "print(\"n_estimator value of best model:\", best_est)\n",
    "print(\"F1 score of the random forest on the model validation set:\", best_score)\n",
    "        \n",
    "best_model = RandomForestClassifier(random_state=12345, n_estimators=best_est)\n",
    "best_model.fit(features_train, target_train)\n",
    "\n",
    "probabilities_valid = best_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print(\"AUC_ROC score of the random forest model on the validation set:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the logistic regression model on the validation set: 0.005376344086021506\n",
      "AUC_ROC score of the logistic regression model on the validation set: 0.646782996530208\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear')  # initialize logistic regression constructor with parameters random_state=54321 and solver='liblinear'\n",
    "model.fit(features_train, target_train)  # train model on training set\n",
    "\n",
    "predictions_valid = model.predict(features_valid)\n",
    "f1_score_lr = f1_score(target_valid, predictions_valid)\n",
    "\n",
    "print(\"F1 score of the logistic regression model on the validation set:\", f1_score_lr)\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print(\"AUC_ROC score of the logistic regression model on the validation set:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best unbalanced model which successfully meets the threshold of an F-score of `0.59` is the Decision Tree model with an F-score of `0.594` on the validation data. We will be refreshing the data and utilizing a few techniques such as scaling, upsampling, and downsampling to apply with the same model types to optimize and improve the model.\n",
    "\n",
    "A cause for these models utilizing unbalanced data could be that with the greatly varying numbers in some of the features, the weight of the predictions is off, causing the relationship between precision and recall represented by the F1-score to be lower than ideal, even if the accuracy of the model is calculated to be relatively high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Great, you tried a few different models and did some hyperparameter tuning. I would suggest instead of optimizing accuracy (which is a poor metric for imbalanced data anyway) to optimize F1 score (which is our target metric in this project). Hyperparameters which maximize one metric are not necessarily the same as hyperparameters maximizing a different metric.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment</b>\n",
    "\n",
    "Whether to use some kind of balancing is a hypeparameter of a model, and thus has to be decided using the validation set. The test set should only be used to evaluate the final model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  I adjusted all the above unbalanced models to work with F1 score optimization for finding parameters. I also utilized validation data for the final scores after fitting them on the training data. The test data is reserved for the final balanced models.\n",
    "    <p></p>\n",
    "I also removed the Decision Tree Regression - clearly I'm still working on figuring out what models go with what.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Great!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying Data for Balanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Why repeat the same code with exactly the same results?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  I can honestly say I do not recall why I repeated previously completed steps unnecessarily other than possibly being very much out of my element with Machine Learning. Likely I did the useless repetition just to feel sure the splits were proper - kind of a measure twice, cut once concept.\n",
    "    <p></p>\n",
    "<p>  I did however remove the extraneous code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Haha, I see :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale features for weight in training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['creditscore', 'age', 'tenure', 'balance', 'numofproducts', 'estimatedsalary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric]) \n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes defined by the `numeric` variable are transformed to be represented by a numerical expression in relation to the standard deviation, square root of the variance. This will ensure they are not weighted disproportionately in the fitting of various models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "> The classes defined by the `numeric` variable are transformed to be represented between the values of `-1` and `1`.\n",
    "    \n",
    "That's not quite what standard scaling does. What it does is subtract the mean value and divide by standard deviation, so the result is not necessarily a value between -1 and 1 (there can be values more than one standard deviation from the mean)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  I adjusted my short post-scaling blurb, hopefully I understand it better and showed as much in my statement. Just in case I'm not mathing this right in my head; this would mean that 99.7% of the data should then fall between -3 and 3, assuming that the rules of a standard distribution apply to the data, while outliers would fall outside of that range. Am I closer than before at least? (It's clearly been a moment since I did any significant SDA)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Yep, that's right! \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling to reduce frequentcy of frequent class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat([features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat([target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state=12345)\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(features_train, target_train, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created downsampled features and target based on the training variables in order to observe frequent classes less in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling to make rare classes less rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)\n",
    "\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created upsampled features and target based on the training variables in order to have the option to observe rarer classes more in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Downsampling and upsampling were correctly applied only to the train set\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The `repeat` and `fraction` parameter seem to be chosen arbitrarily. I would suggest looking at the target distribution to infer the values that would make the data balanced.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  Based on the distribution, I went with `4` and `0.25` for the `repeat` and `fraction` parameters, respectively. Ideally this will get me closer to a good F1 score when using the upsampled and downsampled datasets. But these numbers being off makes sense as to why I was not seeing any realy improvement across the board with my balanced models before. So thank you for the guidance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Indeed, this makes sense! You're welcome :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampled Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of best model: 7\n",
      "F1 score of the decision tree model on the validation set: 0.5599999999999999\n",
      "F1 score of the decision tree model on the test set: 0.5690721649484536\n",
      "AUC_ROC score of the decision tree model on the test set: 0.8248686286526451\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_result = 0\n",
    "for depth in range(1, 50):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model.fit(features_upsampled, target_upsampled)\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    result = f1_score(target_valid, predictions_valid)\n",
    "    if result > best_result:\n",
    "        best_model = model\n",
    "        best_result = result\n",
    "        best_depth = depth\n",
    "        \n",
    "predictions_test = best_model.predict(features_test)\n",
    "test_result = accuracy_score(target_test, predictions_test)\n",
    "\n",
    "print(\"Depth of best model:\", best_depth)\n",
    "print(\"F1 score of the decision tree model on the validation set:\", best_result)\n",
    "\n",
    "f1_score_dt = f1_score(target_test, predictions_test)\n",
    "\n",
    "print(\"F1 score of the decision tree model on the test set:\", f1_score_dt)\n",
    "\n",
    "probabilities_test = best_model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print(\"AUC_ROC score of the decision tree model on the test set:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampled Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of best model: 4\n",
      "F1 score of the decision tree model on the validation set: 0.5586808923375364\n",
      "F1 score of the decision tree model on the test set: 0.5291214215202369\n",
      "AUC_ROC score of the decision tree model on the test set: 0.8084642079439381\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_result = 0\n",
    "for depth in range(1, 50):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model.fit(features_downsampled, target_downsampled)\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    result = f1_score(target_valid, predictions_valid)\n",
    "    if result > best_result:\n",
    "        best_model = model\n",
    "        best_result = result\n",
    "        best_depth = depth\n",
    "        \n",
    "predictions_test = best_model.predict(features_test)\n",
    "test_result = accuracy_score(target_test, predictions_test)\n",
    "\n",
    "print(\"Depth of best model:\", best_depth)\n",
    "print(\"F1 score of the decision tree model on the validation set:\", best_result)\n",
    "\n",
    "f1_score_dt = f1_score(target_test, predictions_test)\n",
    "\n",
    "print(\"F1 score of the decision tree model on the test set:\", f1_score_dt)\n",
    "\n",
    "probabilities_test = best_model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print(\"AUC_ROC score of the decision tree model on the test set:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Alright, here you're maximizing F1 score, very nice!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampled Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator value of best model: 43\n",
      "F1 score of the random forest on the model validation set: 0.6213872832369942\n",
      "F1 score of the random forest model on the test set: 0.5933734939759037\n",
      "AUC_ROC score of the random forest model on the test set: 0.8531602702496289\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 50): \n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators=est) \n",
    "    model.fit(features_upsampled, target_upsampled) \n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    score = f1_score(target_valid, predictions_valid)    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_est = est\n",
    "\n",
    "print(\"n_estimator value of best model:\", best_est)\n",
    "print(\"F1 score of the random forest on the model validation set:\", best_score)\n",
    "        \n",
    "best_model = RandomForestClassifier(random_state=12345, n_estimators=best_est)\n",
    "best_model.fit(features_upsampled, target_upsampled)\n",
    "\n",
    "predictions_test = best_model.predict(features_test)\n",
    "f1_score_rf = f1_score(target_test, predictions_test)\n",
    "\n",
    "print(\"F1 score of the random forest model on the test set:\", f1_score_rf)\n",
    "\n",
    "probabilities_test = best_model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print(\"AUC_ROC score of the random forest model on the test set:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampled Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator value of best model: 14\n",
      "F1 score of the random forest on the model validation set: 0.5846153846153845\n",
      "F1 score of the random forest model on the test set: 0.5730337078651685\n",
      "AUC_ROC score of the random forest model on the test set: 0.8343407775544534\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 50): \n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators=est) \n",
    "    model.fit(features_downsampled, target_downsampled) \n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    score = f1_score(target_valid, predictions_valid)    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_est = est\n",
    "\n",
    "print(\"n_estimator value of best model:\", best_est)\n",
    "print(\"F1 score of the random forest on the model validation set:\", best_score)\n",
    "        \n",
    "best_model = RandomForestClassifier(random_state=12345, n_estimators=best_est)\n",
    "best_model.fit(features_downsampled, target_downsampled)\n",
    "\n",
    "predictions_test = best_model.predict(features_test)\n",
    "f1_score_rf = f1_score(target_test, predictions_test)\n",
    "\n",
    "print(\"F1 score of the random forest model on the test set:\", f1_score_rf)\n",
    "\n",
    "probabilities_test = best_model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print(\"AUC_ROC score of the random forest model on the test set:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment</b>\n",
    "\n",
    "Neither upsampled, nor downsampled data are used to train the decision tree and random forest\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Ok, now you're working both with upsampled and downsampled data. One note: I would still suggest to reserve the test set to only evaluate the model, when you've chosen whether to use upsampling or downsampling using the validation set. Ideally, the type of the model should be selected using the validation set as well to avoid getting a biased estimate of the final model's generalization performance (so then the test set would be used just once: to evaluate the final model).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampled Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the logistic regression model on the test set: 0.4938737040527804\n",
      "AUC_ROC score of the logistic regression model on the test set: 0.7613186125397466\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear')  # initialize logistic regression constructor with parameters random_state=54321 and solver='liblinear'\n",
    "model.fit(features_upsampled, target_upsampled)  # train model on training set\n",
    "\n",
    "predictions_test = model.predict(features_test)\n",
    "f1_score_lr = f1_score(target_test, predictions_test)\n",
    "\n",
    "print(\"F1 score of the logistic regression model on the test set:\", f1_score_lr)\n",
    "\n",
    "probabilities_test = model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print(\"AUC_ROC score of the logistic regression model on the test set:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampled Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the logistic regression model on the test set: 0.49056603773584906\n",
      "AUC_ROC score of the logistic regression model on the test set: 0.7608976281441108\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear')  # initialize logistic regression constructor with parameters random_state=54321 and solver='liblinear'\n",
    "model.fit(features_downsampled, target_downsampled)  # train model on training set\n",
    "\n",
    "predictions_test = model.predict(features_test)\n",
    "f1_score_lr = f1_score(target_test, predictions_test)\n",
    "\n",
    "print(\"F1 score of the logistic regression model on the test set:\", f1_score_lr)\n",
    "\n",
    "probabilities_test = model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print(\"AUC_ROC score of the logistic regression model on the test set:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Alright, the logistic regression model was trained on downsampled data\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment</b>\n",
    "\n",
    "Why use a decision tree regression model for a classification problem? :)\n",
    "    \n",
    "Note that logistic regression is actually a classification model despite its unfortunate name\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  I removed the Decision Tree Regression model, by this point I think I had run each of the previous models at least ten times each so was kind of lost. I'm still very uncomfortable with Machine Learning as a whole, and clearly don't have a great grasp on what models to use when. I also expanded the three remaining models to include both an upscaled and downscaled version to ideally find the best fit.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "Excellent! Yeah, don't worry, you're doing great! :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model for Beta Bank to use in their predictions of what customers will stay or leave is the Random Forest model, utilizing upsampled data, with a depth of 43. \n",
    "\n",
    "Before balancing, the Random Forest model resulted in and F1 Score of 0.587 with depth of 41. This was just under the threshold of `0.59`.\n",
    "\n",
    "After balancing the data and fitting the model, the validation data set resulted in a F1 Score of 0.621, with depth of 43; slightly exceeding the threshold for an acceptable model and showing improvement. When the test data set was processed by the model, we recieved a F1 Score of 0.593 which still meets the minimum threshold. The AUC_ROC score, which can range from 0 to 1, is 0.853 with the testing set, indicating a relatively high quality model as the closer to 1 a model approaches, the greater the quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    As a note: I'm aware that I will likely need edits following my first submission, and will welcome any input and assistance I can get. At this point I'm floundering on how to get my F1 scores higher. I ran the base training sets along with downsampled and upsampled sets through all the models and played with which classes would get scaled to try and get the best results, selecting the best results for all of them at the end. I also tried eliminating various columns such as geography as well to reduce the number of variables in the modelling to no avail. I also adjusted the fraction of the downsampling and multiplier of the upsampling with no luck.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment</b>\n",
    "\n",
    "Yep, there are a couple of problems I noted above. Some suggestions for improving the F1 score:\n",
    "    \n",
    "1. Make sure that upsampled/downsampled data are actually used to train decision tree/random forest\n",
    "2. Use stratification when splitting the data to make sure target distribution in train/validation/test is the same as in the original dataset\n",
    "3. Maybe to enlarge the hyperparameter search space\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  Hopefully this second submission is better. I was able to meet the F1 Score threshold with a model finally. So thank you for all the great advice, adjusting the multiplier and fraction of my up/downscaling along with stratifying the data sets certainly helped. So thiank you again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\n",
    "It is! You're welcome! :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.675px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
